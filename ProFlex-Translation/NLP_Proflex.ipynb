{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2599bd2-11b0-4290-be75-ee01e6db3520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade rouge-score\n",
    "%pip install -q --upgrade keras-nlp\n",
    "%pip install -q --upgrade keras  # Upgrade to Keras 3.\n",
    "\n",
    "#from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371208c2-1e33-42f4-a28b-fd1612606d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import pathlib\n",
    "import random\n",
    "import os\n",
    "# Loss and metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "os.chdir(\"/media/damian/PDB_DB/TRANSLATION/MODEL\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fdd15-d5bf-4e07-9daf-81c0ca38c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 300\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "AA_VOCAB_SIZE = 100\n",
    "PF_VOCAB_SIZE = 100\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ad287-2cdc-45a8-a042-7dfec8fba44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"GLOBAL_150_300_subset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c5038-e6e9-4d98-aeef-4dd5e02b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordmaker_kmer(seq):\n",
    "    k = 1 # length of the k-mer\n",
    "    output = [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "    return \" \".join(output)\n",
    "\n",
    "def wordmaker_non_overlapping(seq, word_length=1):\n",
    "    output = [seq[i:i+word_length] for i in range(0, len(seq), word_length)]\n",
    "    return \" \".join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe8f2b-99b8-430f-8f5c-a34d21906bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    AA, PF = line.split(\",\")\n",
    "    AA = wordmaker_non_overlapping(AA.lower())\n",
    "    PF = wordmaker_non_overlapping(PF)\n",
    "    text_pairs.append((AA, PF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cbe3e-e738-4ecc-a139-77bd10a74e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245b3b3-9450-4eb3-8661-0aa1db8af861",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3166039-0f72-4f27-89f4-25b998533aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cfecc-58fe-435f-b8f1-0674dc553bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "\n",
    "AA_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "AA_vocab = train_word_piece(AA_samples, AA_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "PF_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "PF_vocab = train_word_piece(PF_samples, PF_VOCAB_SIZE, reserved_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabe655-0c5d-4339-97bf-cab43a739aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AA Tokens: \", AA_vocab[10:20])\n",
    "print(\"Proflex Tokens: \", PF_vocab[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863adbd-e582-4e2c-aad8-fdba30a34187",
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=AA_vocab, lowercase=False\n",
    ")\n",
    "PF_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=PF_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906551b-1f8d-4689-bf02-e659adff3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_input_ex = text_pairs[0][0]\n",
    "AA_tokens_ex = AA_tokenizer.tokenize(AA_input_ex)\n",
    "print(\"AA seq: \", AA_input_ex)\n",
    "print(\"Tokens: \", AA_tokens_ex)\n",
    "print(\n",
    "    \"Recovered seq after detokenizing: \",\n",
    "    AA_tokenizer.detokenize(AA_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "PF_input_ex = text_pairs[0][1]\n",
    "PF_tokens_ex = PF_tokenizer.tokenize(PF_input_ex)\n",
    "print(\"PF seq: \", PF_input_ex)\n",
    "print(\"Tokens: \", PF_tokens_ex)\n",
    "print(\n",
    "    \"Recovered seq after detokenizing: \",\n",
    "    PF_tokenizer.detokenize(PF_tokens_ex),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bb6cb-5662-4837-8bb8-2863e9716682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(AA, PF):\n",
    "    batch_size = ops.shape(PF)[0]\n",
    "\n",
    "    AA = AA_tokenizer(AA)\n",
    "    PF = PF_tokenizer(PF)\n",
    "\n",
    "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
    "    AA_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=AA_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    AA = AA_start_end_packer(AA)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to PF and pad it as well.\n",
    "    PF_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=PF_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=PF_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=PF_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    PF = PF_start_end_packer(PF)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": AA,\n",
    "            \"decoder_inputs\": PF[:, :-1],\n",
    "        },\n",
    "        PF[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    AA_texts, PF_texts = zip(*pairs)\n",
    "    AA_texts = list(AA_texts)\n",
    "    PF_texts = list(PF_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((AA_texts, PF_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(1024).prefetch(64).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f41595-454e-4e4a-bbc4-f6dbdfffbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145af96-c863-4105-8e47-68b401d0011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(MAX_SEQUENCE_LENGTH,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=AA_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(MAX_SEQUENCE_LENGTH,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(MAX_SEQUENCE_LENGTH, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=PF_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "decoder_outputs = keras.layers.Dense(PF_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=8,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "# Learning rate schedule\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,  # Lower initial learning rate\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimiser = keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "transformer.summary()\n",
    "# Compile the model\n",
    "transformer.compile(optimizer=optimiser, loss=custom_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954eb00-71e9-456e-8d50-7122631ce276",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()\n",
    "# Compile the model\n",
    "transformer.compile(optimizer=optimiser, loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = transformer.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67341e10-98f1-4c66-aca7-1c3c594e3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "custom_objects = {\n",
    "    'custom_loss': custom_loss,\n",
    "    # Add other custom objects here if needed\n",
    "}\n",
    "\n",
    "model_path = 'model2.keras'\n",
    "transformer = load_model(model_path, custom_objects=custom_objects)\n",
    "#transformer.compile(optimizer=optimiser, loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#%pip install graphviz\n",
    "#import graphviz\n",
    "#plot_model(transformer, to_file='transformer_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71994170-0f96-4bcf-a96a-cb0bb45c4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "def decode_sequences(input_sentences):\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Tokenize the encoder input \n",
    "    encoder_input_tokens = AA_tokenizer(input_sentences)\n",
    "    if isinstance(encoder_input_tokens, tf.RaggedTensor):\n",
    "        encoder_input_tokens = encoder_input_tokens.to_tensor()\n",
    "    encoder_input_tokens = tf.convert_to_tensor(encoder_input_tokens, dtype=tf.int32)\n",
    "    \n",
    "    # Limit input sequence\n",
    "    input_length = tf.shape(encoder_input_tokens)[1]\n",
    "    if input_length > MAX_SEQUENCE_LENGTH:\n",
    "        raise ValueError(\"Input sequence length exceeds MAX_SEQUENCE_LENGTH.\")\n",
    "    \n",
    "    # Pad the input sequence \n",
    "    if input_length < MAX_SEQUENCE_LENGTH:\n",
    "        pads = tf.fill([batch_size, MAX_SEQUENCE_LENGTH - input_length], 0)\n",
    "        encoder_input_tokens = tf.concat([encoder_input_tokens, pads], axis=1)\n",
    "    \n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "\n",
    "    # Build a prompt of length MAX_SEQUENCE_LENGTH with a start token and padding tokens.\n",
    "    start = tf.fill([batch_size, 1], PF_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill([batch_size, MAX_SEQUENCE_LENGTH - 1], PF_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat([start, pad], axis=1)\n",
    "\n",
    "    # Greedy sampling\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        stop_token_ids=[PF_tokenizer.token_to_id(\"[END]\")],\n",
    "        index=1,  # Start sampling after start token.\n",
    "    )\n",
    "    \n",
    "    # Truncate or pad the generated tokens to ensure they match the input length\n",
    "    if tf.shape(generated_tokens)[1] > input_length:\n",
    "        generated_tokens = generated_tokens[:, :input_length]\n",
    "    elif tf.shape(generated_tokens)[1] < input_length:\n",
    "        pads = tf.fill([batch_size, input_length - tf.shape(generated_tokens)[1]], PF_tokenizer.token_to_id(\"[PAD]\"))\n",
    "        generated_tokens = tf.concat([generated_tokens, pads], axis=1)\n",
    "    \n",
    "    # Detokenize the generated tokens to obtain the output sentences.\n",
    "    generated_sentences = PF_tokenizer.detokenize(generated_tokens)\n",
    "    \n",
    "    return generated_sentences\n",
    "\n",
    "# Apply decoder\n",
    "test_AA = [pair[0] for pair in test_pairs if len(pair[0]) <= MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "for i in range(1):\n",
    "    seq = random.choice(test_AA)\n",
    "    translated = decode_sequences([seq])\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f'{seq}')\n",
    "    print(f'{translated}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f1b52-4b17-4a2a-b222-0872c5fb3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "transformer.save('model2.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd206e26-8051-47c0-8997-9c63b24ba7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
